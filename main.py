from telegram.extract_data_tg import ScraperT
from src.summarization.summarizer import PersonalizedSummarizer, TextRankSummarizer
from src.recommendation.vectorizer import NewsVectorizer, UserProfileVectorizer
from src.recommendation.matcher import NewsMatcher
from src.recommendation.user_profile import UserProfileManager
from src.recommendation.report_generator import ReportGenerator
from src.nlp.preprocessing import TextPreprocessor
from src.nlp.regex_annotator import RegexAnnotator
import os 
import json
import numpy as np
import spacy

nlp  = spacy.load('es_core_news_lg')

path = 'Data_articles'
data_dirs = [x for x in os.listdir(path) if not x.startswith(".")]

def load_raw_data(limit=None):
    """Carga datos crudos de art√≠culos"""
    all_data = []
    count = 0
    for data_dir in data_dirs:
        dir_path = os.path.join(path, data_dir)
        for filename in os.listdir(dir_path):
            if filename.endswith('.json'):
                try:
                    with open(os.path.join(dir_path, filename)) as f:
                        article = json.load(f)
                        all_data.append(article)
                        count += 1
                        if limit and count >= limit:
                            return all_data
                except:
                    continue
    return all_data


def clean_article_noise(text: str) -> str:
    """Elimina patrones de ruido como 'LEA TAMBI√âN:."""
    import re
    if not text:
        return ""
    
    # Patrones de referencias a otros art√≠culos
    patterns = [
        r'LEA\s+TAMBI√âN\s*[:.].*?(?=\.\s|$)',
      
    ]
    
    for pattern in patterns:
        text = re.sub(pattern, '', text, flags=re.IGNORECASE)
    
    # Limpiar espacios m√∫ltiples
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text


def process_single_article(args):
    """Procesa un solo art√≠culo (para paralelizaci√≥n con threading)"""
    idx, article_data, nlp, text_processor, annotator = args
    
    try:
        text = article_data.get('text', '')
        if not text:
            return None
        
        # Limpiar ruido antes de procesar
        text = clean_article_noise(text)
        
        # Procesar con spaCy
        doc = nlp(text)
        
        # Extraer entidades
        current_ents = [{'text': e.text, 'label': e.label_} for e in doc.ents]
        
        # Anotar con regex
        annotations = annotator.annotate(text)
        
        # Preprocesar texto
        clean_tokens = text_processor.preprocess(text)
        clean_text = ' '.join(clean_tokens)
        
        return {
            'id': idx,
            'title': article_data.get('title', 'Sin t√≠tulo'),
            'text': text,
            'clean_text': clean_text,
            'categories': annotations['categories'],
            'entidades': current_ents,
            'section': article_data.get('section', 'General'),
            'tags': article_data.get('tags', []),
            'url': article_data.get('url', ''),
            'source_metadata': article_data.get('source_metadata', {}),
        }
    except Exception as e:
        print(f"‚ö†Ô∏è Error procesando art√≠culo {idx}: {e}")
        return None


def prepare_articles(raw_data, text_processor, annotator, news_vectorizer, nlp):
    """
    Prepara art√≠culos: extrae texto, categoriza con regex, limpia y vectoriza
    Usa ThreadPoolExecutor para paralelizaci√≥n real
    
    Returns:
        Lista de art√≠culos procesados con vectores y metadatos
    """
    from concurrent.futures import ThreadPoolExecutor, as_completed
    import tqdm
    import multiprocessing
    
    print(f"\nProcesando {len(raw_data)} art√≠culos...")
    
    # Preparar argumentos para cada art√≠culo
    tasks = [(i, article_data, nlp, text_processor, annotator) 
             for i, article_data in enumerate(raw_data)]
    
    articles = []
    clean_texts = []
    
    
    num_workers = multiprocessing.cpu_count()
    print(f"Procesando en paralelo con {num_workers} threads...")
    
    with ThreadPoolExecutor(max_workers=num_workers) as executor:
        # Enviar todas las tareas
        futures = {executor.submit(process_single_article, task): task[0] 
                   for task in tasks}
        
        # Recopilar resultados con barra de progreso
        for future in tqdm.tqdm(as_completed(futures), total=len(futures)):
            result = future.result()
            if result:
                articles.append(result)
                clean_texts.append(result['clean_text'])
    
    # Ordenar por ID original
    articles.sort(key=lambda x: x['id'])
    
    print(f"‚úÖ {len(articles)} art√≠culos procesados exitosamente")
    
    # Vectorizar todos los textos limpios
    print(f"\nVectorizando art√≠culos con TF-IDF...")
    article_matrix = news_vectorizer.fit_transform0(clean_texts)
    print(f"‚úÖ Matriz de art√≠culos: {article_matrix.shape}")
    
    # Agregar vectores a los art√≠culos
    for i, article in enumerate(articles):
        article['vector'] = article_matrix[i].tolist()
    
    return articles


def save_processed_articles(articles, filepath='processed_articles.json', vectorizer=None):
    """Guarda los art√≠culos procesados y el vectorizador en un √∫nico archivo JSON"""
    print(f"\nüíæ Guardando art√≠culos procesados en {filepath}...")
    
    data = {
         'vectorizer': vectorizer.to_dict() if vectorizer else {},
        'articles': articles
       
    }

    def make_json_serializable(obj):
        """Recursively convert numpy types and other non-JSON types to native Python types."""
        # Numpy scalar
        if isinstance(obj, (np.integer,)):
            return int(obj)
        if isinstance(obj, (np.floating,)):
            return float(obj)
        if isinstance(obj, (np.ndarray,)):
            return obj.tolist()

        # Basic types
        if isinstance(obj, (str, int, float, bool)) or obj is None:
            return obj

        # Datetime
        try:
            from datetime import datetime
            if isinstance(obj, datetime):
                return obj.isoformat()
        except Exception:
            pass

        # Dict
        if isinstance(obj, dict):
            return {str(k): make_json_serializable(v) for k, v in obj.items()}

        # Iterable (list/tuple)
        if isinstance(obj, (list, tuple)):
            return [make_json_serializable(v) for v in obj]

        # Fallback: try to cast to string
        try:
            return str(obj)
        except Exception:
            return None

    serializable_data = make_json_serializable(data)

    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(serializable_data, f, ensure_ascii=False, indent=2)
    print(f"‚úÖ Art√≠culos guardados exitosamente")


def load_processed_articles(filepath='processed_articles.json'):
    """Carga los art√≠culos procesados y vectorizador desde un archivo JSON"""
    if os.path.exists(filepath):
        print(f"\nüìÇ Cargando art√≠culos procesados desde {filepath}...")
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # Compatibilidad con formato antiguo (solo lista de art√≠culos)
        if isinstance(data, list):
            print(f"‚úÖ {len(data)} art√≠culos cargados desde cache (formato antiguo)")
            return {'articles': data, 'vectorizer_data': None}
        
        articles = data.get('articles', [])
        vectorizer_data = data.get('vectorizer', {})
        print(f"‚úÖ {len(articles)} art√≠culos cargados desde cache")
        
        return {'articles': articles, 'vectorizer_data': vectorizer_data}
    return None


def create_simulated_users():
    """Crea perfiles de usuarios simulados enfocados en pol√≠tica latinoamericana"""
    users = [
        {
            'name': 'Mar√≠a - Analista de Pol√≠tica Latinoamericana',
            'profile_text': (
                'Sigo de cerca los procesos pol√≠ticos en Am√©rica Latina, especialmente en Venezuela, '
                'Cuba, Nicaragua, Bolivia y M√©xico. Me interesan los gobiernos progresistas, '
                'el socialismo del siglo XXI y las pol√≠ticas de izquierda. Analizo elecciones, '
                'reformas constitucionales, asambleas nacionales y decisiones del poder ejecutivo. '
                'Sigo a l√≠deres como Maduro, D√≠az-Canel, Petro, AMLO y Lula. Me preocupan '
                'los golpes de estado, la injerencia extranjera y las sanciones de Estados Unidos. '
                'Apoyo la soberan√≠a nacional, la integraci√≥n regional y organismos como CELAC y ALBA.'
            )
        },
        {
            'name': 'Carlos - Corresponsal de Conflictos Internacionales',
            'profile_text': (
                'Cubro conflictos armados, guerras y crisis geopol√≠ticas a nivel mundial. '
                'Me especializo en el conflicto Israel-Palestina, la guerra en Ucrania, '
                'tensiones en Medio Oriente y conflictos en √Åfrica. Denuncio cr√≠menes de guerra, '
                'bombardeos a civiles, uso de armas prohibidas y violaciones del derecho internacional. '
                'Sigo las acciones de la ONU, el Consejo de Seguridad, la Corte Penal Internacional '
                'y organizaciones humanitarias. Me interesan los refugiados, desplazados, '
                'crisis humanitarias y operaciones de paz. Analizo el papel de potencias como '
                'Estados Unidos, Rusia, China e Ir√°n en los conflictos globales.'
            )
        },
        {
            'name': 'Rosa - Defensora de Derechos Humanos',
            'profile_text': (
                'Me dedico a documentar violaciones de derechos humanos en Am√©rica Latina. '
                'Sigo casos de represi√≥n pol√≠tica, presos pol√≠ticos, persecuci√≥n a opositores '
                'y asesinatos de l√≠deres sociales. Me preocupan los pueblos ind√≠genas, '
                'comunidades afrodescendientes, campesinos y trabajadores. Denuncio '
                'la violencia policial, paramilitares, narcotr√°fico y crimen organizado. '
                'Apoyo movimientos sociales, sindicatos, organizaciones de mujeres y colectivos LGBTQ+. '
                'Sigo informes de Amnist√≠a Internacional, Human Rights Watch y la CIDH. '
                'Valoro la justicia social, la memoria hist√≥rica y la verdad sobre dictaduras pasadas.'
            )
        },
        {
            'name': 'Jorge - Economista Pol√≠tico',
            'profile_text': (
                'Analizo la econom√≠a pol√≠tica de Am√©rica Latina y el impacto de las sanciones. '
                'Me interesan las pol√≠ticas econ√≥micas de Venezuela, Cuba y Nicaragua bajo bloqueo. '
                'Sigo el precio del petr√≥leo, la inflaci√≥n, el tipo de cambio y la deuda externa. '
                'Estudio el papel del FMI, Banco Mundial y las pol√≠ticas de austeridad. '
                'Me preocupan la pobreza, la desigualdad, el desempleo y la crisis alimentaria. '
                'Apoyo la nacionalizaci√≥n de recursos, la reforma agraria y la soberan√≠a econ√≥mica. '
                'Analizo tratados comerciales, inversiones chinas y rusas en la regi√≥n, '
                'y alternativas al d√≥lar como moneda de intercambio.'
            )
        },
        {
            'name': 'Luc√≠a - Periodista de Pol√≠tica Electoral',
            'profile_text': (
                'Cubro procesos electorales, campa√±as pol√≠ticas y resultados de votaciones '
                'en toda Am√©rica Latina. Me interesan las elecciones presidenciales, legislativas '
                'y refer√©ndums en Venezuela, Colombia, Brasil, Argentina, M√©xico y Chile. '
                'Analizo encuestas, debates presidenciales, fraudes electorales y observaci√≥n internacional. '
                'Sigo partidos pol√≠ticos de izquierda y derecha, coaliciones y alianzas. '
                'Me preocupa la participaci√≥n ciudadana, el voto electr√≥nico y la transparencia electoral. '
                'Documento victorias progresistas, derrotas de la derecha y cambios de gobierno. '
                'Valoro la democracia, las instituciones electorales y el respeto al voto popular.'
            )
        },
        {
            'name': 'Fernando - Analista Antiimperialista',
            'profile_text': (
                'Estudio las relaciones de poder entre Estados Unidos y Am√©rica Latina. '
                'Denuncio el imperialismo, las intervenciones militares, golpes de estado '
                'y operaciones de cambio de r√©gimen patrocinadas por la CIA. Me interesan '
                'las sanciones econ√≥micas contra Venezuela, Cuba, Nicaragua e Ir√°n. '
                'Sigo las bases militares estadounidenses, el Comando Sur y la OTAN. '
                'Apoyo la multipolaridad, el BRICS, la cooperaci√≥n Sur-Sur y la desdolarizaci√≥n. '
                'Analizo el papel de China y Rusia como contrapeso a la hegemon√≠a estadounidense. '
                'Me preocupan los medios de comunicaci√≥n occidentales y la guerra de informaci√≥n.'
            )
        },
    ]
    return users


def main(nlp):

    print("=" * 80)
    print("SISTEMA DE RECOMENDACI√ìN DE NOTICIAS PERSONALIZADO")
    print("=" * 80)
    
    # Inicializar componentes
    text_processor = TextPreprocessor(use_spacy=False)
    annotator = RegexAnnotator()
    
    # Inicializar vectorizador de noticias (necesario siempre para perfiles de usuario)
    news_vectorizer = NewsVectorizer(max_features=3000, ngram_range=(1, 2))
    
    # Intentar cargar art√≠culos procesados desde cache
    processed_cache_file = 'processed_articles.json'
    cache_data = load_processed_articles(processed_cache_file)
    
    if cache_data is None:
        # No existe cache, procesar art√≠culos desde cero
        print("\nüìÇ Cargando art√≠culos crudos...")
        raw_data = load_raw_data()  # Cambia el limit o qu√≠talo para cargar todos
        print(f"‚úÖ {len(raw_data)} art√≠culos crudos cargados")
        
        # Preparar art√≠culos: categorizar, limpiar y vectorizar
        articles = prepare_articles(raw_data, text_processor, annotator, news_vectorizer, nlp)
        
        # Guardar en cache para futuras ejecuciones
        save_processed_articles(articles, processed_cache_file, vectorizer=news_vectorizer)
    else:
        # Cargar art√≠culos desde cache
        articles = cache_data['articles']
        vectorizer_data = cache_data['vectorizer_data']
        
        if vectorizer_data:
            # Cargar vectorizador desde datos en JSON
            print("\nüîß Restaurando vectorizador desde cache...")
            news_vectorizer = NewsVectorizer.from_dict(vectorizer_data)
            if news_vectorizer:
                print("‚úÖ Vectorizador restaurado")
            else:
                # Fallback si falla la deserializaci√≥n
                print("‚ö†Ô∏è  Error restaurando vectorizador, reajustando...")
                clean_texts = [article['clean_text'] for article in articles]
                news_vectorizer = NewsVectorizer(max_features=3000, ngram_range=(1, 2))
                news_vectorizer.fit0(clean_texts)
                print("‚úÖ Vectorizador ajustado")
        else:
            # Formato antiguo sin vectorizador, necesitamos ajustar
            print("\nüîß Ajustando vectorizador con art√≠culos del cache...")
            clean_texts = [article['clean_text'] for article in articles]
            news_vectorizer.fit0(clean_texts)
            print("‚úÖ Vectorizador ajustado")

    # Crear perfiles de usuarios simulados
    print("\nüë• Creando usuarios simulados...")
    simulated_users = create_simulated_users()
    
    # Inicializar componentes de recomendaci√≥n
    print("\nüìä Inicializando matcher ...")
    profile_vectorizer = UserProfileVectorizer(news_vectorizer)
    profile_manager = UserProfileManager(profile_vectorizer)
    
    # Crear matcher desde art√≠culos
    matcher = NewsMatcher.from_articles(articles, vectorizer=news_vectorizer)
    print(f"‚úÖ Matcher inicializado con {len(articles)} art√≠culos")
    
    # Inicializar resumidores
    base_summarizer = TextRankSummarizer(language="spanish")
    personalized_summarizer = PersonalizedSummarizer(base_summarizer)
    
    # Inicializar generador de reportes
    report_generator = ReportGenerator(personalized_summarizer)
    
    # Procesar cada usuario
    print("\n" + "=" * 80)
    print("GENERANDO RECOMENDACIONES PERSONALIZADAS")
    print("=" * 80)
    
    all_reports = []
    
    # Crear directorio para PDFs
    pdf_output_dir = "reportes_pdf"
    os.makedirs(pdf_output_dir, exist_ok=True)
    
    for user in simulated_users:
        print(f"\n{'='*80}")
        print(f"üë§ Usuario: {user['name']}")
        print(f"{'='*80}")
        print(f"üìù Perfil: {user['profile_text'][:100]}...")
        
        # Crear perfil del usuario con extracci√≥n de entidades
        user_profile = profile_manager.create_profile(user['profile_text'], nlp=nlp)
        
        print(f"\nüè∑Ô∏è  Categor√≠as detectadas: {user_profile['categories'][:10]}")
        
        # Mostrar entidades por tipo
        entities = user_profile.get('entities', [])
        ent_by_type = {}
        for e in entities:
            label = e.get('label', 'MISC')
            if label in {'PER', 'ORG', 'GPE', 'LOC'}:
                if label not in ent_by_type:
                    ent_by_type[label] = []
                ent_by_type[label].append(e['text'])
        
        if ent_by_type:
            print("üîç Entidades extra√≠das:")
            for label, texts in ent_by_type.items():
                label_name = {'PER': 'Personas', 'ORG': 'Organizaciones', 'GPE': 'Pa√≠ses/Ciudades', 'LOC': 'Lugares'}.get(label, label)
                print(f"   {label_name}: {texts[:5]}")
        
        # Encontrar art√≠culos relevantes
        matches = matcher.match_articles(user_profile, articles, top_k=10)
        
        # Generar reporte personalizado
        report = report_generator.generate_report(matches, user_profile, max_articles=5)
        all_reports.append({
            'user_name': user['name'],
            'report': report
        })
        
        import time
        # Generar PDF
        # Crear nombre de archivo seguro
        safe_name = user['name'].replace(' ', '_').replace('-', '_').replace('/', '_')
        pdf_filename = f"{pdf_output_dir}/reporte_{safe_name}_{int(time.time())}.pdf"
        
        print(f"\nüìÑ Generando PDF...")
        if report_generator.generate_pdf(report, pdf_filename, user['name']):
            print(f"‚úÖ PDF guardado en: {pdf_filename}")
        else:
            print(f"‚ö†Ô∏è  No se pudo generar el PDF (instala reportlab: pip install reportlab)")
        
        print(f"\n{'='*80}\n")
    
    # Estad√≠sticas generales
    print("\n" + "=" * 80)
    print("üìä ESTAD√çSTICAS GENERALES")
    print("=" * 80)
    
    
    
    print(f"\nüìÅ Reportes PDF guardados en: {pdf_output_dir}/")
    print("\n‚úÖ Sistema completado exitosamente!")


if __name__ == "__main__":
    main(nlp)
