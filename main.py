from telegram.extract_data_tg import ScraperT
from src.summarization.summarizer import PersonalizedSummarizer, TextRankSummarizer
from src.recommendation.vectorizer import NewsVectorizer, UserProfileVectorizer
from src.recommendation.matcher import NewsMatcher
from src.recommendation.user_profile import UserProfileManager
from src.recommendation.report_generator import ReportGenerator
from src.nlp.preprocessing import TextPreprocessor
from src.nlp.regex_annotator import RegexAnnotator
import os 
import json


path = 'Data_articles'
data_dirs = [x for x in os.listdir(path) if not x.startswith(".")]

def load_raw_data(limit=None):
    """Carga datos crudos de art√≠culos"""
    all_data = []
    count = 0
    for data_dir in data_dirs:
        dir_path = os.path.join(path, data_dir)
        for filename in os.listdir(dir_path):
            if filename.endswith('.json'):
                try:
                    with open(os.path.join(dir_path, filename)) as f:
                        article = json.load(f)
                        all_data.append(article)
                        count += 1
                        if limit and count >= limit:
                            return all_data
                except:
                    continue
    return all_data


def prepare_articles(raw_data, text_processor, annotator, news_vectorizer):
    """
    Prepara art√≠culos: extrae texto, categoriza con regex, limpia y vectoriza
    
    Returns:
        Lista de art√≠culos procesados con vectores y metadatos
    """
    articles = []
    clean_texts = []
    
    print(f"\nüì∞ Procesando {len(raw_data)} art√≠culos...")
    import tqdm 
    for i, article_data in enumerate(tqdm.tqdm(raw_data)):
        try:
            # Extraer texto
            text = article_data.get('text', '')
            if not text:
                continue
            
            # Anotar con regex para extraer categor√≠as
            annotations = annotator.annotate(text)
            
            # Preprocesar texto
            clean_tokens = text_processor.preprocess(text)
            clean_text = ' '.join(clean_tokens)
            clean_texts.append(clean_text)
            
            # Guardar art√≠culo procesado (sin vector a√∫n)
            articles.append({
                'id': i,
                'title': article_data.get('title', 'Sin t√≠tulo'),
                'text': text,
                'clean_text': clean_text,
                'categories': annotations['categories'],
                'section': article_data.get('section', 'General'),
                'tags': article_data.get('tags', []),
                'url': article_data.get('url', ''),
                'source_metadata': article_data.get('source_metadata', {}),
            })
            
        except Exception as e:
            continue
    
    print(f"‚úÖ {len(articles)} art√≠culos procesados exitosamente")
    
    # Vectorizar todos los textos limpios
    print(f"\nüî¢ Vectorizando art√≠culos con TF-IDF...")
    article_matrix = news_vectorizer.fit_transform0(clean_texts)
    print(f"‚úÖ Matriz de art√≠culos: {article_matrix.shape}")
    
    # Agregar vectores a los art√≠culos
    for i, article in enumerate(articles):
        article['vector'] = article_matrix[i].tolist()
    
    return articles


def save_processed_articles(articles, filepath='processed_articles.json'):
    """Guarda los art√≠culos procesados en un archivo JSON"""
    print(f"\nüíæ Guardando art√≠culos procesados en {filepath}...")
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(articles, f, ensure_ascii=False, indent=2)
    print(f"‚úÖ Art√≠culos guardados exitosamente")


def load_processed_articles(filepath='processed_articles.json'):
    """Carga los art√≠culos procesados desde un archivo JSON"""
    if os.path.exists(filepath):
        print(f"\nüìÇ Cargando art√≠culos procesados desde {filepath}...")
        with open(filepath, 'r', encoding='utf-8') as f:
            articles = json.load(f)
        print(f"‚úÖ {len(articles)} art√≠culos cargados desde cache")
        return articles
    return None


def create_simulated_users():
    """Crea perfiles de usuarios simulados con diferentes intereses basados en categor√≠as regex"""
    users = [
        {
            'name': 'Sof√≠a - Cr√≠tica de Arte',
            'profile_text': (
                'Soy una apasionada del arte contempor√°neo, las exposiciones y las galer√≠as. '
                'Me interesan las obras de artistas emergentes, el muralismo, la escultura y '
                'la fotograf√≠a art√≠stica. Sigo festivales culturales, bienales de arte, '
                'inauguraciones de museos y eventos de patrimonio cultural. Me fascina el '
                'teatro, la danza, el cine de autor y las manifestaciones art√≠sticas urbanas. '
                'Disfruto la m√∫sica cl√°sica, jazz, y expresiones folcl√≥ricas tradicionales.'
            )
        },
        {
            'name': 'Diego - Ambientalista',
            'profile_text': (
                'Me dedico a la conservaci√≥n ambiental y protecci√≥n de ecosistemas. '
                'Sigo temas de biodiversidad, especies en peligro de extinci√≥n, reservas naturales '
                'y parques nacionales. Me preocupan los desastres naturales como terremotos, '
                'inundaciones y huracanes. Denuncio la deforestaci√≥n, contaminaci√≥n de r√≠os, '
                'derrames de petr√≥leo y el cambio clim√°tico. Apoyo energ√≠as renovables, '
                'reciclaje y desarrollo sostenible. Me interesan proyectos de reforestaci√≥n '
                'y la protecci√≥n de oc√©anos y recursos h√≠dricos.'
            )
        },
        {
            'name': 'Laura - Educadora Cultural',
            'profile_text': (
                'Me apasiona la educaci√≥n, la literatura y la promoci√≥n cultural. '
                'Sigo lanzamientos de libros, ferias literarias, conciertos y recitales de poes√≠a. '
                'Me interesan programas educativos, becas, talleres art√≠sticos y actividades '
                'para ni√±os y j√≥venes. Apoyo bibliotecas comunitarias, centros culturales '
                'y espacios de creaci√≥n art√≠stica. Me gusta el teatro comunitario, '
                'la m√∫sica folcl√≥rica y las tradiciones ancestrales. Valoro la preservaci√≥n '
                'del patrimonio inmaterial y las lenguas ind√≠genas.'
            )
        },
        {
            'name': 'Mart√≠n - Fot√≥grafo de Naturaleza',
            'profile_text': (
                'Soy fot√≥grafo especializado en naturaleza, paisajes y vida silvestre. '
                'Me apasionan los parques naturales, santuarios de fauna, volcanes y monta√±as. '
                'Documento especies animales, aves migratorias, flora end√©mica y ecosistemas √∫nicos. '
                'Me interesan expediciones cient√≠ficas, descubrimientos de nuevas especies '
                'y proyectos de conservaci√≥n de h√°bitats. Sigo fen√≥menos naturales, auroras, '
                'eclipses y eventos astron√≥micos. Apoyo el turismo ecol√≥gico y responsable.'
            )
        },
        {
            'name': 'Carmen - Historiadora del Arte',
            'profile_text': (
                'Investigo historia del arte latinoamericano, arquitectura colonial y '
                'patrimonio hist√≥rico. Me fascinan las restauraciones de monumentos, '
                'excavaciones arqueol√≥gicas y descubrimientos de sitios hist√≥ricos. '
                'Estudio arte prehisp√°nico, culturas ind√≠genas y tradiciones artesanales. '
                'Me interesan museos, archivos hist√≥ricos, documentales culturales '
                'y la preservaci√≥n de arte sacro. Valoro el arte popular, textiles tradicionales '
                'y t√©cnicas ancestrales de pintura y cer√°mica.'
            )
        },
    ]
    return users


def main():

    print("=" * 80)
    print("SISTEMA DE RECOMENDACI√ìN DE NOTICIAS PERSONALIZADO")
    print("=" * 80)
    
    # Inicializar componentes
    text_processor = TextPreprocessor(use_spacy=False)
    annotator = RegexAnnotator()
    
    # Intentar cargar art√≠culos procesados desde cache
    processed_cache_file = 'processed_articles.json'
    articles = load_processed_articles(processed_cache_file)
    
    if articles is None:
        # No existe cache, procesar art√≠culos desde cero
        print("\nüìÇ Cargando art√≠culos crudos...")
        raw_data = load_raw_data()  # Cambia el limit o qu√≠talo para cargar todos
        print(f"‚úÖ {len(raw_data)} art√≠culos crudos cargados")
        
        # Inicializar vectorizador de noticias
        news_vectorizer = NewsVectorizer(max_features=3000, ngram_range=(1, 2))
        
        # Preparar art√≠culos: categorizar, limpiar y vectorizar
        articles = prepare_articles(raw_data, text_processor, annotator, news_vectorizer)
        
        # Guardar en cache para futuras ejecuciones
        save_processed_articles(articles, processed_cache_file)
    
    
    # Crear perfiles de usuarios simulados
    print("\nüë• Creando usuarios simulados...")
    simulated_users = create_simulated_users()
    
    # Inicializar componentes de recomendaci√≥n
    profile_vectorizer = UserProfileVectorizer(news_vectorizer)
    profile_manager = UserProfileManager(profile_vectorizer)
    matcher = NewsMatcher()
    
    # Inicializar resumidores
    base_summarizer = TextRankSummarizer(language="spanish")
    personalized_summarizer = PersonalizedSummarizer(base_summarizer)
    
    # Inicializar generador de reportes
    report_generator = ReportGenerator(personalized_summarizer)
    
    # Procesar cada usuario
    print("\n" + "=" * 80)
    print("GENERANDO RECOMENDACIONES PERSONALIZADAS")
    print("=" * 80)
    
    all_reports = []
    
    # Crear directorio para PDFs
    pdf_output_dir = "reportes_pdf"
    os.makedirs(pdf_output_dir, exist_ok=True)
    
    for user in simulated_users:
        print(f"\n{'='*80}")
        print(f"üë§ Usuario: {user['name']}")
        print(f"{'='*80}")
        print(f"üìù Perfil: {user['profile_text'][:100]}...")
        
        # Crear perfil del usuario
        user_profile = profile_manager.create_profile(user['profile_text'])
        
        print(f"\nüè∑Ô∏è  Categor√≠as de inter√©s detectadas: {user_profile['categories'][:8]}")
        print(f"üìä Dimensi√≥n del vector de perfil: {len(user_profile['vector'])}")
        
        # Encontrar art√≠culos relevantes
        matches = matcher.match_articles(user_profile, articles, top_k=10)
        
        # Generar reporte personalizado
        report = report_generator.generate_report(matches, user_profile, max_articles=5)
        all_reports.append({
            'user_name': user['name'],
            'report': report
        })
        
        # Generar PDF
        # Crear nombre de archivo seguro
        safe_name = user['name'].replace(' ', '_').replace('-', '_').replace('/', '_')
        pdf_filename = f"{pdf_output_dir}/reporte_{safe_name}.pdf"
        
        print(f"\nüìÑ Generando PDF...")
        if report_generator.generate_pdf(report, pdf_filename, user['name']):
            print(f"‚úÖ PDF guardado en: {pdf_filename}")
        else:
            print(f"‚ö†Ô∏è  No se pudo generar el PDF (instala reportlab: pip install reportlab)")
        
        print(f"\n{'='*80}\n")
    
    # Estad√≠sticas generales
    print("\n" + "=" * 80)
    print("üìä ESTAD√çSTICAS GENERALES")
    print("=" * 80)
    
    # Categor√≠as m√°s comunes en art√≠culos
    category_counts = {}
    for article in articles:
        for cat in article['categories']:
            category_counts[cat] = category_counts.get(cat, 0) + 1
    
    print("\nüèÜ Top 10 categor√≠as m√°s frecuentes en art√≠culos:")
    sorted_cats = sorted(category_counts.items(), key=lambda x: x[1], reverse=True)[:10]
    for cat, count in sorted_cats:
        print(f"   {cat}: {count} art√≠culos")
    
    print(f"\nüìÅ Reportes PDF guardados en: {pdf_output_dir}/")
    print("\n‚úÖ Sistema completado exitosamente!")


if __name__ == "__main__":
    main()
